<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI Voice Chat</title>
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap"
      rel="stylesheet"
    />
    <link href="/static/style.css" rel="stylesheet" />
  </head>
  <body>
    <div class="container">
      <h2>ðŸŽ¤ AI Voice Chat</h2>
      <div class="text" id="userText">(Push to Talk)</div>
      <div class="text" id="aiReply">Brian AI: Question me.</div>
      <button class="button" id="micBtn">Hold to Speak</button>
      <div class="loading" id="loading" style="display: none">Thinking...</div>
    </div>

    <script>
      let micBtn = document.getElementById("micBtn");
      let userText = document.getElementById("userText");
      let aiReply = document.getElementById("aiReply");
      let loading = document.getElementById("loading");
      let mediaRecorder,
        audioChunks = [];

      let naturalVoice = null;

      const utterance = new SpeechSynthesisUtterance(chatData.reply);
      utterance.lang = "en-US";
      utterance.rate = 1;
      utterance.pitch = 1;

      function loadVoices() {
        const voices = speechSynthesis.getVoices();

        console.log(speechSynthesis.getVoices());
        // à¹€à¸¥à¸·à¸­à¸à¹€à¸ªà¸µà¸¢à¸‡à¸—à¸µà¹ˆà¸Ÿà¸±à¸‡à¸”à¸¹à¸˜à¸£à¸£à¸¡à¸Šà¸²à¸•à¸´ (à¸‚à¸¶à¹‰à¸™à¸à¸±à¸š OS/Browser)
        naturalVoice = voices.find(
          (v) => v.name.includes("Andrew") // v.name.includes("Google") ||
          // v.name.includes("Samantha") || v.name.includes("Alex") ||
          // v.name.includes("Microsoft") || v.name.includes("Zira") ||
          // v.name.includes("David") || v.name.includes("Microsoft Server") ||
          // v.name.includes("Microsoft Zira") || v.name.includes("Microsoft David") ||
          // v.lang === "en-US" || v.lang === "en-GB" ||
          // v.name.includes("Microsoft") ||
        );
      }

      // à¸šà¸²à¸‡ browser à¸•à¹‰à¸­à¸‡à¸£à¸­à¹ƒà¸«à¹‰ voices à¹‚à¸«à¸¥à¸”à¹€à¸ªà¸£à¹‡à¸ˆà¸à¹ˆà¸­à¸™
      if (speechSynthesis.onvoiceschanged !== undefined) {
        speechSynthesis.onvoiceschanged = loadVoices;
      }

      // à¸£à¸­à¸‡à¸£à¸±à¸šà¸—à¸±à¹‰à¸‡ desktop (mouse) à¹à¸¥à¸° mobile (touch)
      function startRecording() {
        if (micBtn.disabled) return;

        userText.innerHTML = `Listening`;
        navigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {
          mediaRecorder = new MediaRecorder(stream, { mimeType: "audio/webm" });
          audioChunks = [];

          mediaRecorder.ondataavailable = (e) => audioChunks.push(e.data);

          mediaRecorder.onstop = async () => {
            micBtn.disabled = true;
            micBtn.classList.remove("recording");
            loading.style.display = "block";

            const blob = new Blob(audioChunks, { type: "audio/webm" });
            const formData = new FormData();
            formData.append("file", blob, "audio.webm");

            // STEP 1: à¸ªà¹ˆà¸‡à¹€à¸ªà¸µà¸¢à¸‡à¹„à¸›à¹ƒà¸«à¹‰ whisper à¸—à¸µà¹ˆ backend
            const res = await fetch("/transcribe", {
              method: "POST",
              body: formData,
            });
            const data = await res.json();
            const transcript = data.transcript?.trim();

            if (!transcript) {
              userText.innerHTML = "You: <em>(no voice recognized)</em>";
              micBtn.disabled = false;
              loading.style.display = "none";
              return;
            }

            userText.innerHTML = `You: <strong>${transcript}</strong>`;

            // STEP 2: à¸ªà¹ˆà¸‡ transcript à¹„à¸›à¸«à¸² AI chat
            const chatForm = new FormData();
            chatForm.append("message", transcript);
            const chatRes = await fetch("/chat", {
              method: "POST",
              body: chatForm,
            });
            const chatData = await chatRes.json();

            if (naturalVoice) {
              console.log(naturalVoice);
            }
            utterance.voice = naturalVoice;

            speechSynthesis.speak(utterance);

            aiReply.innerHTML = `AI: ${chatData.reply}`;
            micBtn.disabled = false;
            loading.style.display = "none";
          };

          micBtn.classList.add("recording");
          mediaRecorder.start();
        });
      }

      function stopRecording() {
        if (mediaRecorder && mediaRecorder.state === "recording") {
          mediaRecorder.stop();
        }
      }

      // Desktop
      micBtn.addEventListener("mousedown", startRecording);
      micBtn.addEventListener("mouseup", stopRecording);

      // Mobile
      micBtn.addEventListener("touchstart", (e) => {
        e.preventDefault(); // à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸à¸²à¸£ select/copy
        startRecording();
      });

      micBtn.addEventListener("touchend", (e) => {
        e.preventDefault(); // à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸à¸²à¸£ select/copy
        stopRecording();
      });

      // âœ… fallback just in case
      // window.onload = () => {
      //   loadVoices();
      // };
    </script>
  </body>
</html>
